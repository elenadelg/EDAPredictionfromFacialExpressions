# Predicting Electrodermal Activity from Facial Expresssion Analysis

## ğŸ“ Introduction

This project **investigates ElectroDermal Activity (EDA) prediction using video-based facial expression analysis**. The study evaluates and compares several **deep learning architectures**, including **PhysNet, DeepPhys, and CNN-based baselines**, to explore their effectiveness in predicting EDA under real-world conditions using the UBFC-Phys dataset. It explores:
- Deep learning models performance for predicting EDA from facial videos.
- Data augmentation and face cropping effectiveness in reducing overfitting and improving generalization.
- Assess generalization capabilities and explore limitations in handling small, noisy datasets.


---

## ğŸ“Š Dataset: UBFC-Phys

The UBFC-Phys Dataset ([Sabour et al., 2021][1]) was used, which contains:
- **Participants:** 56 individuals aged 19 to 38.
- **Data streams:** 
  - Synchronized video frames (4 FPS)
  - EDA signals (4 Hz)
  - Blood Volume Pulse (BVP) signals (64 Hz)
- **Tasks:**  
  Each participant was recorded during three tasks simulating different physiological states:
  - Resting
  - Speech
  - Arithmetic tests

---

### âš™ï¸  Models 

#### 1. **Sequential CNNs**
- A **Sequential CNN** learns temporal patterns from facial video frames and integrates **LSTMs** to capture dependencies over time.
- **Sequential CNN + BVP:** Incorporates BVP signals alongside video data to enhance prediction accuracy.

#### 2. **PhysNet** ([Yu et al., 2019][2])
- Employs **spatio-temporal 3D convolutions** to capture blood volume variations across facial frames.
- Enhanced to handle BVP signals for better prediction of EDA.

#### 3. **DeepPhys**  ([Chen & McDuff, 2018][3])
- Incorporates **attention mechanisms** to focus on physiologically relevant regions, such as skin areas.
- Processes appearance and motion tensors** via a dual-design **CNN architecture**.

---

### ğŸ“ Evaluation Metrics
- **Mean Squared Error (MSE):** Measures absolute accuracy by quantifying the difference between actual and predicted EDA signals.
- **Negative Pearson Correlation (NPC):** Evaluates the linear correlation between actual and predicted EDA signals.  
- **Grad-CAM:** Visualizes model attention, highlighting the facial regions that contribute the most to EDA predictions. ([Selvaraju et al., 2017][4]).

---

## ğŸ”¬ Experiments

- **Data Augmentation**: Random transformations (e.g., rotations, color jitter, flips) help models learn generalized patterns and reduce overfitting.
- **Face Cropping**:  Face regions are isolated using the **face_recognition library** ([Ageitgey, 2018][5]), allowing the models to focus on meaningful features and reduce noise.

---

## âš ï¸ Challenges and Limitations

### 1. **Data Quality and Diversity**
- The small dataset (56 participants) and **gender imbalance** limited model generalization.
- **Systematic bias** was introduced due to homogeneous recording environments.

### 2. **Limited Temporal Resolution**
- Downsampling video from **35 FPS to 4 FPS** reduced the available temporal context, limiting the ability to capture subtle variations in facial expressions.

---
## ğŸ” **References**

[1] Sabour, R. M., Benezeth, Y., De Oliveira, P., Chappe, J., & Yang, F. (2021). "UBFC-Phys: A multimodal database for psychophysiological studies of social stress." IEEE Transactions on Affective Computing.

[2] Yu, Z., Li, X., Zhao, G., & PietikÃ¤inen, M. (2019). "Remote Photoplethysmograph Signal Measurement From Facial Videos Using Spatio-Temporal Convolutional Networks." Proceedings of the AAAI Conference on Artificial Intelligence.

[3] Chen, W., & McDuff, D. (2018). "DeepPhys: Video-Based Physiological Measurement Using Convolutional Attention Networks." European Conference on Computer Vision (ECCV).

[4] Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2017). "Grad-CAM: Visual explanations from deep networks via gradient-based localization." Proceedings of the IEEE International Conference on Computer Vision (ICCV).

[5] Ageitgey, A. (2018). "face_recognition: The worldâ€™s simplest facial recognition API for Python and the command line." GitHub repository: https://github.com/ageitgey/face_recognition.



---

## ğŸ› ï¸ Repository Structure

```plaintext
/CV
    â”œâ”€â”€ notebooks/                        # Jupyter notebooks for exploratory data analysis
    â”‚   â”œâ”€â”€ exploratorydataanalysis.ipynb  
    â”‚   â””â”€â”€ summary_patients.ipynb         
    â”‚
    â”œâ”€â”€ src/                              # Source code for the project
    â”‚   â”œâ”€â”€ data/                         
    â”‚   â”‚   â”œâ”€â”€ datacollection/            
    â”‚   â”‚   â”œâ”€â”€ dataloader/                
    â”‚   â”‚   â””â”€â”€ dataprocessing/            
    â”‚   â”‚
    â”‚   â”œâ”€â”€ utils.py                      # Utility functions for eda tasks
    â”‚
    â”‚   â”œâ”€â”€ evaluation/                   
    â”‚   â”‚   â”œâ”€â”€ gradcam/                   # Grad-CAM visualizations
    â”‚   â”‚   â””â”€â”€ loss/                      # Loss computation and NPC evaluation
    â”‚   â”‚
    â”‚   â”œâ”€â”€ models/                       # Core model architectures
    â”‚   â”‚   â”œâ”€â”€ DeepPhys.py                
    â”‚   â”‚   â”œâ”€â”€ PhysNet.py                 
    â”‚   â”‚   â”œâ”€â”€ seqCNN_BVP.py              
    â”‚   â”‚   â””â”€â”€ seqCNN.py                  
    â”‚   â”‚
    â”‚   â”œâ”€â”€ evaluate.py                   
    â”‚   â””â”€â”€ training.py                   
    â”‚
    â”œâ”€â”€ .python-version                  
    â”œâ”€â”€ pyproject.toml                   
    â”œâ”€â”€ README.md                         
    â”œâ”€â”€ requirements.txt                  
    â””â”€â”€ JobParameters.json                
